---
title: STT Approach Analysis - Real-time vs Batch for Voice Coding
created: 07-02-2026
tags: [research, stt, whisper, vad, voice-coding, architecture]
---

# STT Approach Analysis: Real-time vs Batch for Voice Coding

**Research Date:** 07 February 2026

## Coverage Key
<span style="color:#3498db">ğŸŒ</span> Global | <span style="color:#2ecc71">ğŸ‡ºğŸ‡¸</span> United States

---

## Executive Summary

For STVC (Simple Text-to-Voice for Claude), **Hybrid VAD + Batch is the clear winner** for voice coding use cases:

âœ… **Recommended Architecture:**
- **Silero VAD** for automatic speech detection (hands-free, 4x more accurate than WebRTC VAD)
- **faster-whisper** with batch processing for transcription (4x faster than OpenAI Whisper)
- **No hotkey needed** - VAD detects when user starts/stops speaking
- **0.5-1s latency** is acceptable since user waits for Claude anyway
- **92-99% accuracy** <span style="color:#3498db">ğŸŒ</span> for technical vocabulary (critical for coding)

<span style="color:#ff6b6b;font-weight:bold">Real-time streaming NOT recommended</span> due to:
- Lower accuracy (partial results flicker/change)
- Higher complexity with minimal UX benefit
- Whisper wasn't designed for streaming (requires hacky workarounds)

---

## Query: Real-time Streaming vs Batch STT for Voice Coding

Voice coding requires dictating technical instructions like:
- "Add a function that validates email addresses and returns a boolean"
- "Refactor the authentication module to use JWT tokens"
- Typical utterance: 5-30 seconds
- Accuracy matters more than speed (wrong transcription = wrong code)

---

## Approach Comparison

### 1. Batch/Push-to-Talk (PTT) <span style="color:#f1c40f">âš ï¸ Acceptable but outdated</span>

**How it works:**
- User holds hotkey, speaks, releases
- Entire audio clip sent to Whisper at once
- Text appears after processing delay

**Pros:**
- Highest accuracy (92-99% for technical language) <span style="color:#3498db">ğŸŒ</span>
- Simplest implementation
- Works perfectly with OpenAI Whisper

**Cons:**
- Requires hands on keyboard (breaks "vibe coding" flow)
- Perceived latency (user waits for transcription)
- Manual start/stop not natural for conversational flow

**Performance:**
- Whisper accuracy: 92-95% (whisper mode), 97.2% (normal volume) <span style="color:#3498db">ğŸŒ</span>
- Latency: 0.5-1s for 5-30s clips

---

### 2. Real-time Streaming <span style="color:#ff6b6b">âŒ NOT Recommended</span>

**How it works:**
- Audio processed in chunks as user speaks
- Partial results appear immediately
- Final result may differ from partials

**Tools:**
- [whisper-streaming](https://github.com/ufal/whisper_streaming) (UFAL implementation)
- [whisper-flow](https://github.com/dimastatz/whisper-flow) (incremental transcripts)
- faster-whisper with custom streaming wrapper

**Pros:**
- Feels instant with progressive feedback
- Sub-500ms latency achievable <span style="color:#3498db">ğŸŒ</span>

**Cons:**
- <span style="color:#ff6b6b;font-weight:bold">Lower accuracy</span> - streaming sacrifices accuracy for speed
- <span style="color:#ff6b6b;font-weight:bold">Partial result flickering</span> - text changes as context builds
- <span style="color:#ff6b6b;font-weight:bold">Whisper not designed for streaming</span> - requires workarounds
- Much higher implementation complexity
- Buffer management is tricky

**Critical Issue for Voice Coding:**
Wrong partial results could trigger Claude to start generating incorrect code before the full utterance is processed. Accuracy is paramount.

**Performance:**
- Latency: 300-500ms (P50) for streaming APIs <span style="color:#3498db">ğŸŒ</span>
- Accuracy: Lower than batch (research shows "streaming reduces latency but affects accuracy")
- Whisper-Streaming achieves 3.3s latency with "self-adaptive latency" <span style="color:#3498db">ğŸŒ</span>

---

### 3. Hybrid: VAD + Batch <span style="color:#2ecc71;font-weight:bold">âœ… RECOMMENDED</span>

**How it works:**
- Voice Activity Detection (VAD) continuously monitors microphone
- Automatically detects when user starts speaking
- Buffers audio while speaking
- Detects silence (user finished)
- Automatically sends complete audio to Whisper for batch transcription
- No hotkey needed (hands-free)

**Best-in-class Tools:**

| Component | Tool | Why |
|-----------|------|-----|
| VAD | **Silero VAD** | 4x fewer errors than WebRTC VAD, 87.7% TPR @ 5% FPR <span style="color:#3498db">ğŸŒ</span> |
| Transcription | **faster-whisper** | 4x faster than OpenAI Whisper, same accuracy <span style="color:#3498db">ğŸŒ</span> |
| Optimization | **Distil-Whisper** | 6x faster than Whisper Large V3, within 1% WER <span style="color:#3498db">ğŸŒ</span> |

**Silero VAD Performance:**
- **87.7% True Positive Rate** at 5% False Positive Rate <span style="color:#3498db">ğŸŒ</span>
- WebRTC VAD: 50% TPR (misses 1 in 2 speech frames)
- Silero: Misses only 1 in 8 speech frames
- Processes 30ms+ audio in <1ms on single CPU thread <span style="color:#3498db">ğŸŒ</span>
- Trained on 6000+ languages, handles noisy environments <span style="color:#3498db">ğŸŒ</span>

**faster-whisper Performance:**
- **4x faster** than OpenAI Whisper (same accuracy) <span style="color:#3498db">ğŸŒ</span>
- **12.5x speedup** with VAD batching + optimized feature extraction <span style="color:#3498db">ğŸŒ</span>
- Built-in VAD support (removes silence >2s automatically)
- Lower memory usage
- Supports quantization (8-bit) for even faster inference

**Pros:**
- âœ… **Hands-free** - no hotkey needed
- âœ… **Highest accuracy** - full batch processing with context
- âœ… **Natural interaction** - speak whenever ready
- âœ… **Home office friendly** - Silero VAD handles background noise well
- âœ… **Fast enough** - 0.5-1s latency acceptable for voice coding workflow
- âœ… **Simple architecture** - VAD + batch (no streaming complexity)

**Cons:**
- Slight delay after user stops speaking (silence threshold)
- Need to tune silence threshold (typically 0.5-1.5s)

**Implementation Strategy:**
```python
# Pseudocode
while True:
    audio_chunk = microphone.read()

    if silero_vad.is_speech(audio_chunk):
        buffer.append(audio_chunk)
        last_speech_time = now()

    # User stopped speaking for 1 second
    if buffer.has_audio() and (now() - last_speech_time > 1.0):
        full_audio = buffer.get_all()
        text = faster_whisper.transcribe(full_audio)
        send_to_claude(text)
        buffer.clear()
```

---

### 4. Hybrid: Streaming + Final Correction <span style="color:#f1c40f">âš ï¸ Complexity Not Worth It</span>

**How it works:**
- Stream partial results in real-time for immediate feedback
- Replace with final corrected text when user finishes speaking
- "Best of both worlds" in theory

**Reality Check:**
- Adds significant complexity (streaming + batch pipeline)
- Partial results can confuse user/Claude
- For voice coding, seeing "add a funk shun" â†’ "add a function" is distracting
- Minimal UX benefit since user waits for Claude response anyway
- Not worth engineering effort

---

## Voice Activity Detection (VAD) Deep Dive

### What is VAD?

Voice Activity Detection answers: **"Is someone speaking right now?"**

Simple energy-based VAD measures audio amplitude, but fails in noisy environments. Modern VAD uses deep learning to distinguish speech from:
- Background noise (HVAC, traffic, music)
- Non-speech sounds (coughing, keyboard typing, mouse clicks)
- Distant/reverberant speech

### VAD Comparison <span style="color:#3498db">ğŸŒ</span>

| Feature | **Silero VAD** (Recommended) | WebRTC VAD | Cobra VAD (Picovoice) |
|---------|------------------------------|------------|------------------------|
| **Approach** | Deep learning (DNN) | GMM signal processing | Deep learning |
| **Accuracy @ 5% FPR** | 87.7% TPR | 50% TPR | >95% TPR (claimed) |
| **Error Rate** | Baseline | 4x more errors | <1 false alarm/hour |
| **Background Noise** | Excellent | Poor | Excellent |
| **Latency** | <1ms per 30ms chunk | ~10ms | Very low |
| **Languages** | 6000+ | Language-agnostic | Language-agnostic |
| **License** | MIT (Open Source) | BSD (Open Source) | Commercial (free tier) |
| **Complexity** | Easy integration | Very simple | Easy integration |

**Winner:** <span style="color:#2ecc71;font-weight:bold">Silero VAD</span> - Best accuracy, open source, excellent noise handling

### Tuning VAD for Home Office

**Silence Threshold:**
- Default: 0.5-1.0 seconds of silence = "user finished speaking"
- Too short: Cuts off during pauses ("add a function... that validates emails")
- Too long: User waits unnecessarily after finishing

**Activation Threshold:**
- Range: 0.0 to 1.0 (higher = louder speech needed)
- Default: 0.5 (medium-loud speech)
- Noisy environment: 0.6-0.7 (requires louder speech)
- Quiet room: 0.3-0.4 (more sensitive)

**Pre/Post Speech Padding:**
- Add 200-300ms before speech start (catch beginning)
- Add 300-500ms after speech end (catch trailing words)

**Recommended Config for Voice Coding:**
```python
vad_config = {
    'threshold': 0.5,           # Medium sensitivity
    'min_speech_duration': 0.3, # Ignore clicks < 300ms
    'min_silence_duration': 1.0, # 1s pause = finished
    'speech_pad_ms': 300,        # Pad before/after speech
}
```

---

## Wake Word Detection (Alternative to Hotkeys)

### What is Wake Word Detection?

Trigger voice assistant with a spoken phrase (like "Hey Claude" or "Computer") instead of pressing a hotkey.

**Use Case for STVC:**
- User says "Hey Claude" â†’ VAD activates â†’ Speak instruction â†’ Transcribe â†’ Send to Claude
- Completely hands-free workflow

### Wake Word Solutions <span style="color:#3498db">ğŸŒ</span>

| Solution | Type | Performance | Custom Wake Words | License |
|----------|------|-------------|-------------------|---------|
| **Porcupine** (Picovoice) | On-device DL | >95% TPR, <1 false alarm/hour | Yes (train in seconds) | Free tier + paid |
| **Sensory TrulyHandsfree** | On-device | Enterprise-grade | Yes | Enterprise |
| **openWakeWord** | Open source | Good for common words | Pre-trained models | Apache 2.0 |
| **Rhasspy** | Open source | Community-driven | Yes | MIT |

**Winner:** <span style="color:#2ecc71;font-weight:bold">Porcupine</span> - Custom wake words, instant training, self-service

**Porcupine Features:**
- Train custom wake words in seconds (no coding needed)
- On-device processing (no cloud, privacy-friendly)
- Low latency (instant activation without network round-trip)
- Runs on any platform (embedded, mobile, desktop)
- Multiple wake words supported with minimal CPU/memory footprint

### Wake Word vs Push-to-Talk Trade-offs

| Aspect | Wake Word | Push-to-Talk |
|--------|-----------|--------------|
| **Hands-free** | âœ… Yes | âŒ No |
| **False activations** | âš ï¸ Possible (1/hour) | âœ… Never |
| **Latency** | +200-500ms (detection) | Instant |
| **Complexity** | Higher | Lower |
| **User experience** | Futuristic, natural | Manual, reliable |

**Recommendation for STVC v1.0:**
- Start with **PTT (push-to-talk)** for reliability
- Add **wake word** as optional feature in v1.1+
- Let users choose their preferred activation method

---

## Whisper Model Variants Performance <span style="color:#3498db">ğŸŒ</span>

| Model | Speed vs Base | WER vs Base | Memory | Best For |
|-------|---------------|-------------|---------|----------|
| **OpenAI Whisper** | 1x (baseline) | Baseline | High | Maximum accuracy, multilingual |
| **faster-whisper** | 4x faster | Same | Medium | CPU/modest GPU, production use |
| **Distil-Whisper Large V3** | 6.3x faster | Within 1% | Low | Low-latency streaming |
| **Whisper Turbo** | ~3x faster | Comparable | Medium | OpenAI API users |

**Noise Robustness:**
- faster-whisper: 1.3x fewer repeated 5-grams, 2.1% lower insertion error rate
- Handles background noise better than base Whisper

**Recommendation for STVC:**
Use **faster-whisper** with **Distil-Large-V3 model** for optimal speed/accuracy balance.

---

## Technical Architecture Recommendation

### STVC Optimal Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MICROPHONE (Continuous Audio Stream)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SILERO VAD (Real-time Speech Detection)    â”‚
â”‚  - Threshold: 0.5                            â”‚
â”‚  - Min silence: 1.0s                         â”‚
â”‚  - Processes 30ms chunks in <1ms             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AUDIO BUFFER                                â”‚
â”‚  - Captures speech segments                  â”‚
â”‚  - Adds 300ms pre/post padding               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼ (User stops speaking)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FASTER-WHISPER (Batch Transcription)        â”‚
â”‚  - Model: distil-large-v3                    â”‚
â”‚  - Latency: 300-700ms                        â”‚
â”‚  - Accuracy: 92-99%                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CLAUDE CODE (Process Instruction)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Implementation Libraries

**Python Stack:**
```python
# VAD
import torch
from silero_vad import load_silero_vad

# Transcription
from faster_whisper import WhisperModel

# Audio capture
import sounddevice as sd  # or pyaudio

# Optional: Wake word
import pvporcupine  # Picovoice Porcupine
```

**Installation:**
```bash
pip install torch silero-vad faster-whisper sounddevice
# Optional: pip install pvporcupine
```

---

## Voice Coding Accuracy Requirements

### Why Accuracy is Critical

Voice coding dictation requires **>95% accuracy** for usable experience:

âŒ **Wrong transcription examples:**
- "add a funk shun" instead of "add a function"
- "return a boolean" â†’ "return a bouillon"
- "JWT tokens" â†’ "JW tea tokens"
- "refactor the auth module" â†’ "we factor the off module"

**Impact:**
- Wrong transcription = Wrong code generated by Claude
- User must edit text or re-dictate (breaks flow)
- Debugging code from bad transcription is frustrating

### Technical Vocabulary Challenge

Voice coding includes:
- Framework names (React, Django, TensorFlow)
- Programming terms (async/await, boolean, refactor)
- Variable naming (camelCase, snake_case mentions)
- File paths and extensions

**Whisper Performance on Technical Language:**
- General speech: 92-95% accuracy <span style="color:#3498db">ğŸŒ</span>
- Technical vocabulary: Modern AI tools achieve 50%+ higher accuracy than built-in systems <span style="color:#2ecc71">ğŸ‡ºğŸ‡¸</span>
- Wispr Flow (specialized voice coding tool): Native IDE integration for coding context <span style="color:#2ecc71">ğŸ‡ºğŸ‡¸</span>

**Batch vs Streaming for Accuracy:**
<span style="color:#ff6b6b;font-weight:bold">Batch processing wins</span> - full utterance context improves disambiguation:
- "add a function that..." (context helps recognize "function" vs "funk shun")
- Streaming processes chunks without full context
- Research confirms: "streaming sacrifices accuracy for speed"

---

## User Experience Considerations

### What Users Care About (Voice Coding)

**Priority 1: Accuracy** (95%+ required)
- Wrong transcription = wrong code
- Must handle technical vocabulary
- <span style="color:#2ecc71">Batch wins</span>

**Priority 2: Natural workflow**
- Hands-free preferred (no hotkey)
- Speak when ready, no manual start/stop
- <span style="color:#2ecc71">VAD + Batch wins</span>

**Priority 3: Latency** (acceptable: <1.5s)
- User waits for Claude to process/respond anyway
- 0.5-1s transcription delay not noticeable
- <span style="color:#2ecc71">Batch acceptable</span>

**Priority 4: Visual feedback**
- Show "Listening..." indicator when VAD detects speech
- Show "Transcribing..." during Whisper processing
- Show final text before sending to Claude
- <span style="color:#f1c40f">Easy to add with any approach</span>

### Workflow Comparison

**Batch PTT (Current approach):**
1. User presses hotkey âŒ¨ï¸
2. Speak instruction ğŸ—£ï¸
3. Release hotkey âŒ¨ï¸
4. Wait 0.5-1s â±ï¸
5. See transcription ğŸ“
6. Claude processes ğŸ¤–

**VAD + Batch (Recommended):**
1. Speak instruction ğŸ—£ï¸ (hands-free)
2. Auto-detects silence âœ…
3. Wait 0.5-1s â±ï¸
4. See transcription ğŸ“
5. Claude processes ğŸ¤–

**Streaming (Not recommended):**
1. Speak instruction ğŸ—£ï¸
2. See partial results (flickering) ğŸ“âš¡ï¸ğŸ“âš¡ï¸
3. See final result ğŸ“
4. Claude processes ğŸ¤–

**UX Winner:** <span style="color:#2ecc71;font-weight:bold">VAD + Batch</span> - Natural, accurate, hands-free

---

## Recommendations for STVC

### Phase 1: MVP (Recommended Starting Point)

**Architecture:**
- **Silero VAD** for automatic speech detection
- **faster-whisper** (distil-large-v3 model) for batch transcription
- **No streaming** (complexity not worth it)

**Why this stack:**
- âœ… Hands-free (no hotkey needed)
- âœ… Highest accuracy (92-99% for technical language)
- âœ… Fast enough (0.5-1s latency acceptable)
- âœ… Simple implementation (VAD + batch)
- âœ… Open source (MIT/BSD licenses)
- âœ… Runs locally (privacy-friendly, no API costs)

**Implementation priorities:**
1. Silero VAD integration (speech detection)
2. Audio buffer management (capture speech segments)
3. faster-whisper transcription (batch processing)
4. Visual feedback ("Listening...", "Transcribing...")
5. Silence threshold tuning (0.8-1.2s for voice coding)

**Expected performance:**
- Latency: 300-700ms transcription + 1.0s silence detection = **1.0-1.7s total**
- Accuracy: **95-99%** with proper VAD + full context batch processing
- CPU usage: Minimal (VAD <1ms per 30ms, Whisper runs on batch)

### Phase 2: Enhancements (Future Iterations)

**Optional features:**
1. **Wake word activation** (Porcupine) - "Hey Claude" instead of auto-detection
2. **Custom vocabulary injection** - Boost recognition of project-specific terms
3. **Multi-microphone support** - Better noise rejection with beam forming
4. **Adaptive silence threshold** - ML-based adjustment to user speech patterns
5. **Voice commands** - "undo", "send", "cancel" spoken commands

**NOT recommended:**
- âŒ Real-time streaming (accuracy sacrifice not worth it)
- âŒ Cloud APIs (privacy concerns, API costs, network dependency)
- âŒ Multiple transcription engines (complexity, maintenance burden)

### Why NOT Streaming?

<span style="color:#ff6b6b;font-weight:bold">Streaming is wrong solution for voice coding:</span>

| Issue | Impact on STVC |
|-------|----------------|
| Lower accuracy | Wrong transcription = wrong code (unacceptable) |
| Partial results flicker | Distracting, confusing for user |
| Whisper not designed for it | Requires hacky workarounds, maintenance burden |
| Minimal UX benefit | User waits for Claude anyway (0.5s saved not meaningful) |
| Much higher complexity | More code, more bugs, harder to debug |

**When streaming DOES make sense:**
- Live captioning (user needs to see words immediately)
- Real-time translation (speaker still talking)
- Voice assistants (need to interrupt mid-sentence)
- Call center analytics (real-time sentiment analysis)

**Voice coding is different:**
- User dictates complete instruction (5-30s)
- Waits for Claude to process (seconds to minutes)
- Accuracy > Speed
- **Batch + VAD is optimal architecture**

---

## Key Metrics Summary <span style="color:#3498db">ğŸŒ</span>

### VAD Performance
- **Silero VAD:** 87.7% TPR @ 5% FPR, 4x fewer errors than WebRTC VAD
- **Processing time:** <1ms per 30ms audio chunk (single CPU thread)
- **Languages:** 6000+ supported
- **False positives:** ~4% with proper threshold tuning

### Transcription Performance
- **faster-whisper:** 4x faster than OpenAI Whisper (same accuracy)
- **Distil-Whisper Large V3:** 6.3x faster, within 1% WER
- **Latency:** 300-700ms for 5-30s audio clips
- **Accuracy:** 92-99% for clear speech, 95%+ for technical language with context

### Batch vs Streaming Trade-offs
- **Batch accuracy:** 97.2% (full context) <span style="color:#3498db">ğŸŒ</span>
- **Streaming accuracy:** Lower (research confirms accuracy sacrifice)
- **Batch latency:** 0.5-1.5s (includes silence detection)
- **Streaming latency:** 300-500ms (partial results, not final)

### Voice Coding Requirements
- **Minimum accuracy:** 95% for usable experience
- **Acceptable latency:** <2s (user waits for Claude anyway)
- **Technical vocabulary:** Critical (framework names, programming terms)
- **Hands-free:** Preferred (no hotkey interruption)

---

## Alternative: Commercial APIs (Not Recommended for STVC)

### Why Local Processing Wins

**Privacy:**
- Code instructions often reference proprietary codebases
- Sending audio to cloud APIs = potential data leak
- Local processing = zero data leaves machine

**Cost:**
- Cloud APIs: $0.006-0.024 per minute <span style="color:#2ecc71">ğŸ‡ºğŸ‡¸</span>
- Heavy voice coding: 50-100 requests/day = $9-72/month
- Local processing: Free after initial setup

**Latency:**
- Cloud: Network round-trip + processing = 500ms-2s
- Local: Processing only = 300-700ms

**Reliability:**
- Cloud: Network dependency (fails offline)
- Local: Works anywhere, anytime

### When Cloud APIs Make Sense

âœ… Use cloud APIs if:
- No local GPU (Whisper requires compute)
- Need enterprise support/SLA
- Multi-language support critical
- Don't want to manage models

âŒ For STVC specifically:
- Local GPU available (modern laptops have sufficient compute)
- Open source models work great (Whisper, faster-whisper)
- Privacy important (code = intellectual property)
- No recurring costs preferred

---

## Implementation Checklist

### Core Components

- [ ] **Silero VAD integration**
  - [ ] Load model (`load_silero_vad()`)
  - [ ] Configure threshold (0.5 default)
  - [ ] Configure silence duration (1.0s for voice coding)
  - [ ] Add pre/post speech padding (300ms)

- [ ] **Audio capture**
  - [ ] Set up microphone input (sounddevice/pyaudio)
  - [ ] Configure sample rate (16kHz for Whisper)
  - [ ] Implement circular buffer for VAD chunks
  - [ ] Implement speech segment buffer

- [ ] **faster-whisper integration**
  - [ ] Download distil-large-v3 model
  - [ ] Configure model parameters (language="en", task="transcribe")
  - [ ] Implement batch transcription endpoint
  - [ ] Handle transcription errors gracefully

- [ ] **Visual feedback UI**
  - [ ] "Ready" indicator (idle state)
  - [ ] "Listening..." indicator (VAD detected speech)
  - [ ] "Transcribing..." indicator (Whisper processing)
  - [ ] Display transcription result
  - [ ] Confirm/edit before sending to Claude

- [ ] **Integration with Claude Code**
  - [ ] Send transcription as text input
  - [ ] Handle Claude responses
  - [ ] Display Claude output

### Testing & Tuning

- [ ] **Test VAD accuracy**
  - [ ] Test in quiet room (baseline)
  - [ ] Test with background music
  - [ ] Test with HVAC/fan noise
  - [ ] Test with keyboard typing noise
  - [ ] Tune threshold for environment

- [ ] **Test transcription accuracy**
  - [ ] Test common programming terms
  - [ ] Test framework names (React, Django, etc.)
  - [ ] Test file path dictation
  - [ ] Test variable naming patterns
  - [ ] Measure WER (Word Error Rate)

- [ ] **Test latency**
  - [ ] Measure VAD detection time
  - [ ] Measure transcription time (various lengths)
  - [ ] Measure end-to-end latency
  - [ ] Optimize buffer sizes if needed

- [ ] **Test edge cases**
  - [ ] Very short utterances (<2s)
  - [ ] Very long utterances (>30s)
  - [ ] Multiple sentences with pauses
  - [ ] False activations (non-speech sounds)
  - [ ] Silence detection edge cases

### Future Enhancements (Phase 2)

- [ ] **Wake word detection** (Porcupine)
  - [ ] Choose wake phrase ("Hey Claude", "Computer")
  - [ ] Train custom model (Picovoice Console)
  - [ ] Integrate wake word â†’ VAD pipeline
  - [ ] Add wake word settings UI

- [ ] **Custom vocabulary**
  - [ ] Extract technical terms from codebase
  - [ ] Boost Whisper recognition for project terms
  - [ ] User-defined vocabulary additions

- [ ] **Adaptive silence threshold**
  - [ ] Track user speech patterns
  - [ ] ML-based auto-adjustment
  - [ ] Per-user profile storage

---

## Sources

### STT Architecture & Performance
- [Best Speech-to-Text APIs in 2026: A Comprehensive Comparison Guide](https://deepgram.com/learn/best-speech-to-text-apis-2026)
- [Why Enterprises Are Moving to Streaming â€” and Why Whisper Can't Keep Up](https://deepgram.com/learn/why-enterprises-are-moving-to-streaming-and-why-whisper-can-t-keep-up)
- [WhisperX vs Competitors: AI Transcription Comparison 2026](https://brasstranscripts.com/blog/whisperx-vs-competitors-accuracy-benchmark)
- [Best open source speech-to-text (STT) model in 2026 (with benchmarks)](https://northflank.com/blog/best-open-source-speech-to-text-stt-model-in-2026-benchmarks)
- [Top APIs and models for real-time speech recognition and transcription in 2026](https://www.assemblyai.com/blog/best-api-models-for-real-time-speech-recognition-and-transcription)

### faster-whisper & VAD Integration
- [GitHub - SYSTRAN/faster-whisper: Faster Whisper transcription with CTranslate2](https://github.com/SYSTRAN/faster-whisper)
- [Faster Whisper Transcription: How to Maximize Performance for Real-Time Audio-to-Text](https://www.cerebrium.ai/articles/faster-whisper-transcription-how-to-maximize-performance-for-real-time-audio-to-text)
- [How to Implement High-Speed Voice Recognition in Chatbot Systems with WhisperX & Silero-VAD](https://medium.com/@aidenkoh/how-to-implement-high-speed-voice-recognition-in-chatbot-systems-with-whisperx-silero-vad-cdd45ea30904)
- [What is VAD and Diarization With Whisper Models (A Complete Guide)?](https://www.f22labs.com/blogs/what-is-vad-and-diarization-with-whisper-models-a-complete-guide/)
- [Generally Available: The fastest, most accurate and cost-efficient Whisper transcription](https://www.baseten.co/blog/the-fastest-most-accurate-and-cost-efficient-whisper-transcription/)

### VAD Comparison & Performance
- [Choosing the Best Voice Activity Detection in 2026: Cobra vs Silero vs WebRTC VAD](https://picovoice.ai/blog/best-voice-activity-detection-vad/)
- [Voice Activity Detection (VAD): The Complete 2026 Guide to Speech Detection](https://picovoice.ai/blog/complete-guide-voice-activity-detection-vad/)
- [GitHub - snakers4/silero-vad: Silero VAD: pre-trained enterprise-grade Voice Activity Detector](https://github.com/snakers4/silero-vad)
- [GitHub - gkonovalov/android-vad: Android Voice Activity Detection (VAD) library](https://github.com/gkonovalov/android-vad)
- [One Voice Detector to Rule Them All](https://thegradient.pub/one-voice-detector-to-rule-them-all/)

### Whisper Streaming Implementations
- [GitHub - ufal/whisper_streaming: Whisper realtime streaming for long speech-to-text transcription](https://github.com/ufal/whisper_streaming)
- [Turning Whisper into Real-Time Transcription System](https://arxiv.org/html/2307.14743)
- [Zero to real-time transcription: The complete Whisper V3 streaming tutorial](https://www.baseten.co/blog/zero-to-real-time-transcription-the-complete-whisper-v3-websockets-tutorial/)
- [GitHub - dimastatz/whisper-flow: Framework for real-time audio transcription](https://github.com/dimastatz/whisper-flow)

### Voice Coding Accuracy & Requirements
- [AI Dictation for Developers & Coders: Can You Really Code by Voice in 2026?](https://speechify.com/blog/ai-dictation-for-developers-coders-voice-coding-2026/)
- [Wispr Flow 2026 Review: AI Voice Dictation & Auto-Editing](https://max-productive.ai/ai-tools/wispr-flow/)
- [AI Voice Dictation for Coding: 4x Your Speed in 2025](https://willowvoice.com/blog/ai-voice-dictation-coding-speed-2025)
- [Best real-time speech-to-text apps in 2026](https://www.assemblyai.com/blog/best-real-time-speech-to-text-apps)

### Wake Word Detection
- [Porcupine Wake Word Detection & Keyword Spotting - Picovoice](https://picovoice.ai/platform/porcupine/)
- [Wake Word Detection Guide 2026: Complete Technical Overview](https://picovoice.ai/blog/complete-guide-to-wake-word/)
- [GitHub - Picovoice/porcupine: On-device wake word detection powered by deep learning](https://github.com/Picovoice/porcupine)
- [Creating a Custom Wake Word with Porcupine](https://picovoice.ai/blog/console-tutorial-custom-wake-word/)
- [GitHub - dscripka/openWakeWord: Open-source audio wake word detection framework](https://github.com/dscripka/openWakeWord)

### Whisper Model Variants
- [Choosing between Whisper variants: faster-whisper, insanely-fast-whisper, WhisperX](https://modal.com/blog/choosing-whisper-variants)
- [GitHub - huggingface/distil-whisper: Distilled variant of Whisper - 6x faster, 50% smaller](https://github.com/huggingface/distil-whisper)
- [Whisper Variants Comparison: Features And Implementation](https://pub.towardsai.net/whisper-variants-comparison-what-are-their-features-and-how-to-implement-them-c3eb07b6eb95)

### Voice UX & Workflow Design
- [Future Of UI UX Design: 2026 Trends & New AI Workflow](https://motiongility.com/future-of-ui-ux-design/)
- [Why Immersive And Voice Interfaces Are The Next UX Frontier](https://raw.studio/blog/why-immersive-and-voice-interfaces-are-the-next-ux-frontier/)
- [Voice-Enabled AI Workflow Builder: What Actually Works in 2025](https://blog.dograh.com/voice-enabled-ai-workflow-builder-what-actually-works-in-2025/)

---

## Conclusion

<span style="color:#2ecc71;font-weight:bold">For STVC, the winning architecture is unambiguous:</span>

**Silero VAD + faster-whisper (batch processing)**

This combination delivers:
- âœ… **95-99% accuracy** (critical for code generation)
- âœ… **Hands-free operation** (natural workflow)
- âœ… **0.5-1.5s latency** (acceptable for voice coding)
- âœ… **Simple architecture** (easy to implement, maintain)
- âœ… **Privacy-friendly** (local processing)
- âœ… **Zero API costs** (open source models)

<span style="color:#ff6b6b;font-weight:bold">Real-time streaming is NOT recommended</span> because:
- âŒ Lower accuracy (unacceptable for code)
- âŒ Minimal UX benefit (user waits for Claude anyway)
- âŒ Significantly higher complexity
- âŒ Whisper wasn't designed for streaming

Start simple with VAD + batch, then add optional enhancements like wake words in future iterations. The architecture scales well and provides the best balance of accuracy, user experience, and implementation simplicity.
